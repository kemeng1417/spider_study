scrapy异步爬虫框架

一、什么是Scrapy?
    scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架
    集成各种功能：高性能异步下载、队列、分布式、解析、持久化等
    通用性强

二、安装
    Linux
        pip3 install scrapy

    windows：
        pip3 install wheel
        下载twisted http://www.lfd/uci.edu/~gohlke/pythonlibs/#twisted
        进入下载目录安装
        pip3 install pywin32
        pip3 install scrapy

三、基本使用
    - 创建一个工程
        - scrapy startproject proName
        - 工程目录下的两个文件表示的含义
            - spiders包：爬虫文件夹
                - 必须在爬虫文件夹中创建一个爬虫源文件
            - settings.py 配置文件
    - cd proName
        - 创建一个爬虫文件（爬虫文件要创建在spiders包中）
        - scrapy genspider spiderName www.example.com
        - 爬虫文件可以创建多个，名称不能重复
    - 编写爬虫文件
    - 执行工程
        - scrapy crawl spiderName

四、爬虫文件的编写
    - 定义好了一个类，该类的父类是Spider，是scrapy所有类的父类
    - 类中定义三个属性和一个方法：
        - name：爬虫文件的名称
        - start_urls:起始url列表
            - 作用：可以对列表中的url进行get请求的发送
        - allow_domains:允许的域名
        - parse(self,response):
            - 将起始url请求成功后，response就是获取的响应的对象
            可以在该方法中实现数据解析
    - scrapy工程默认是遵从robots协议，需要在配置中关闭
        - 改为不遵从
        - 指定日志等级
            - LOG_LEVEL = 'ERROR'

五、数据解析
    - response.xpath()
    - 注意：
        - 提取标签内容时，返回的不是字符串，而是Selector对象，字符串是存储在该对象中的
        - 需要调用extract()或者extract_first()将Selector对象中的字符串取出

六、持久化存储
    - 基于终端指令：
        - 只可以将parse方法的返回值写入到指定后缀的文本文件中
        - 通过指定方式执行工程：
            - scrapy crawl duanzi -o data_duanzi.csv
    - 基于管道：
        - 实现流程
            - 1.在爬虫文件中解析数据
            - 2.在Item类中定义相关的属性（解析的数据有几个字段就定义几个属性）
            - 3.将在爬虫文件中解析的数据存储封装到Item对象中
            - 4.将存储了解析数据的Item对象提交给管道
            - 5.在管道文件中接收Item对象，且对其进行任意形式的持久化存储操作
            - 6.在配置文件中开启管道
    - 如何实现数据的备份
        - 将数据存储到多个不同的空间中（文件、mysql、redis）
        - 持久化存储的操作必须存储在管道文件中
            - 一个管道类对应一种形式的持久化存储
                - 如果想将数据存储到多个载体中，必须有多个管道类
        - 问题：让两个管道类都接收到item且对其进行持久化存储，爬虫文件提交的item是否可以同时提交给两个管道类？
            - 爬虫文件提交的item只能提交给优先级最高的管道类
        - 如何让优先级管道类也可以获取接收到item呢？
            - 可以让优先级高的管道类的process_item中通过return item 的形式将item返回给下一个即将被执行的管道类


七、手动请求发送实现的全站数据爬取
    - 通过代码手动发起请求
        - yield scrapy.Request(url, callback)->get请求
        - yield scrapy.FormRequest(url, formdata, callback)->post请求
        - 注意：在scrapy中一般不发送post请求

请求传参
    - 作用：实现深度爬取
    - 深度爬取：存在不同页面中的数据
    - 实现：
        - yield scrapy.Request(url,callback,meta={'item':item})
            - 可以将meta字典传递给callback
        - callback接收meta：
            - meta = response.meta

- 五大核心组件
    - 引擎（scrapy）
        - 用来接收整个系统的数据流，触发事务（框架核心）
    - 调度器（Scheduler)
        - 用来接收引擎发送过来的请求，压入队列中，并在引擎再次请求时返回，决定了队列的优先级，决定下一个url是什么，同时去除重复的url
    - 下载器（downloader)
        - 用于下载网页内容，并将网页内容返回scrapy，时建立在twisted上的高效的异步框架
    - 爬虫（spider）
        - 用于在网页中提取有用的信息，即item，用户可以提出链接，让他继续抓取下一个页面
    - 项目管道（pipeline）
        - 负责处理爬虫从网页中抽取的实体，主要功能是持久化存储、验证实体的有效性，清除不需要的信息，当页面被爬虫解析后，发送到管道，并进行几个特定次序的处理。


- scrapy配置文件settings.py其他配置
    增加并发
        - 默认scrapy开启的并发是16个，可以适当的增加，在settings.py中修改CONCURRENT_REQUESTS=100,并设置成100
    降低日志级别
        - 在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率，可以设置log输出信息为INFO或者ERROR即可，在配置中编写：LOG_LEVEL='ERROR'
    禁止cookie
        - 如果不需要cookie，则在scrapy爬取数据时禁止cookie从而减少cpu的使用，提升爬虫的效率，COOKIES_ENABLED = False
    禁止重试
        -  对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False
     减少下载超时：
        如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s



- 中间件
    - 下载中间件
    - 爬虫中间件
    - 作用：
        - 下载中间件是处于引擎和下载器之间。批量拦截请求和响应
    - 拦截请求
        - 请求头的伪装
            - process_request:
                - request.headers['xxx'] = 'xxx'  局部的UA伪装
        - 代理
            - process_exception:
                - request.meta['proxy'] = 'http://ip:port'


        - process_request:每次发起请求，请求都会被该方法拦截到
        - process_response:只要返回一个响应，都会被该方法拦截到
        - process_exception:只要有异常的请求，就会被该方法拦截到，该方法负责对异常的请求进行修正，且通过return request将修
            正后的请求重新发送
    - 拦截响应
        - 篡改响应数据（不重要）



- CrawlSpider
    - 是spider的一个子类，之前创建的爬虫文件的爬虫类的父类就是Spider
    - 作用：用于实现全站数据爬取
    - 使用：
        - 创建工程
        - cd进入工程目录中
        - 创建爬虫文件：
            - scrapy genspider -t crawl spidername www.xx.com
        - 链接提取器，规则解析器，是CrawlSpider独有的
        - 链接提取器LinkExtractor
            - 作用：根据指定规则（allow）进行链接的提取
        - 规则解析器
            - 作用：可以接收链接提取器提取到链接，且对其进行请求发送，然后根据指定形式callback进行数据解析
        - 执行工程


    - 注意
        - 注意：不使用请求传参持久化存储可以实现，但是无法实现数据一一对应。
        - crawlSpider通常和手动传参搭配使用。
        - 一个链接提取器对应一个规则解析器
        - 可以有多个链接提取器和规则解析器
        - 使用CrawlSpider实现深度爬取最好结合手动请求一起发送
        - 如何使用CrawlSpider将一个网站中所有的链接捕获到？
            - LinkExtract(allow=''),将正则匹配的规则设为空

    - follow=True
        - 让链接提取器继续作用到链接提取器到的链接，所对应的页面中

- 分布式
    - 概念：使用多台机器搭建一个分布式机器，在分布式机群中共同运行同一组程序，让其对同一个网络资源进行联合数据爬取
    - 原生的Scrapy无法实现分布式？
        - 调度器无法被分布式机群共享
        - 管道无法共享
    - 如何实现分布式？
        - 使用scrapy结合scrapy-redis组件实现分布式
    - scrapy-redis组件使用：
        - 给scrapy提供可以被共享的管道和调度器
        - pip install scrapy-redis
    - 实现流程
        1.创建工程
        2.cd 工程
        3.创建爬虫文件
        4.修改爬虫文件
            - 导包：from scrapy_redis.spiders import RedisCrawlSpider
            - 修改当前爬虫类的父类：
                - RedisCrawlSpider
            - 删除start_urls，添加一个新属性：
                - redis_key = 'xxx'，可以被共享的调度器队列的名称
            - 基于常规的操作获取url发送请求解析数据
        5.修改配置文件：settings
            - 指定调度器：
                # 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化
                DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
                # 使用scrapy-redis组件自己的调度器
                SCHEDULER = "scrapy_redis.scheduler.Scheduler"
                # 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据
                SCHEDULER_PERSIST = True
            - 指定管道：
                ITEM_PIPELINES = {
                'scrapy_redis.pipelines.RedisPipeline': 400
            }

            - 指定redis服务器：
                REDIS_HOST = 'redis服务的ip地址'
                REDIS_PORT = 6379
        6.对redis的配置文件进行配置（redis.windows.conf）
            - 取消默认绑定
                - 将bind 127.0.0.1进行注释
            - 关闭保护模式
                - protected-mode yes：将yes改为no
        7.启动redis的服务器和客户端
            - redis服务端的启动：
                - 在终端里执行：redis-server.exe redis.windows.conf
            - 启动客户端：redis-cli

        8.执行分布式程序
            - 在日志输出可以看出日志hang住了，为啥会hang住，因为在等待起始的url
        9.向调度器的队列中扔入一个起始的url：
            - 在redis-cli：lpush fbsQueue http://wz.sun0769.com/political/index/politicsNewest?id=1
            - proName:items命名的数据结构中

- 增量式
    - 概念：检测网站数据更新的情况，爬取最新更新出来的数据
    - 核心：去重
    - 实现增量：
        - 对爬取的数据url进行监测，使用一个记录表存储爬取数据的url。如果记录表中已存在，说明已经爬取过，否则表示没有爬取过为新数据。
        - 记录表实现:
            - redis的set集合充当记录表
            - 注意：3.0以上的redis无法存储字典和item，可以将redis的版本降低使用