{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开发环境介绍\n",
    "- anaconda:基于数据分析和机器学习的集成环境\n",
    "- jupyter notebook:基于浏览器的可视化开发工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本使用 \n",
    "- 终端中输入jupyter notebook 启动\n",
    "- 进入的默认结构为终端对应的目录结构\n",
    "- new->python3\n",
    "- cell一个编辑行\n",
    "    - code模式 编写并执行代码\n",
    "    - markdown模式 支持html标签，笔记\n",
    "- 插入 a：上方插入 b： 下方插入\n",
    "- 删除 dd 或 x\n",
    "- 执行 shift+enter\n",
    "- 切换：\n",
    "    - y 变成code模式\n",
    "    - m 变成markdown模式\n",
    "- 打开帮助文档 shift+tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是爬虫\n",
    "- 爬虫：编写程序，**模拟**浏览器上网，**抓取**所需要的数据\n",
    "- 分类：\n",
    "    - 通用爬虫\n",
    "        - 爬取一整张页面\n",
    "    - 聚焦爬虫\n",
    "        - 局部的数据\n",
    "    - 增量式爬虫\n",
    "        - 能一直监测网站的变化，爬取到最新更新的数据\n",
    "    - 分布式爬虫\n",
    "- 反爬机制\n",
    "    - 设定一些机制，禁止爬取数据\n",
    "        - rpbots协议：君子协议，规定哪些可以爬取，哪些不可以爬取\n",
    "- 反反爬策略\n",
    "    - 制定相关的策略破解反爬机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests模块\n",
    "- urllib模块：模拟浏览器上网，发送网络请求\n",
    "- requests模块：基于网络请求的模块\n",
    "\n",
    "- 编码流程：\n",
    "    - 指定url发送请求\n",
    "    - 发起请求\n",
    "    - 获取相应数据\n",
    "    - 持久化存储\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.爬取搜狗首页的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.sogou.com/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}\n",
    "responce = requests.get(url=url, headers=headers)\n",
    "page_text = responce.text\n",
    "with open('./sougou.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入搜索的关键字：希林娜依·高\n",
      "下载成功\n"
     ]
    }
   ],
   "source": [
    "# 乱码处理\n",
    "word = input('输入搜索的关键字：')\n",
    "url = 'https://www.sogou.com/web'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
    "}\n",
    "# 动态传参\n",
    "params = {\n",
    "    'query': word\n",
    "}\n",
    "responce = requests.get(url=url, headers=headers, params=params)\n",
    "page_text = responce.text\n",
    "filename = word+'.html'\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(page_text)\n",
    "\n",
    "print('下载成功')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 异常的访问请求\n",
    "    - 通过浏览器发起的请求，爬虫发起的请求是异常\n",
    "- 区别：\n",
    "    - 请求头 User-Agent:请求载体的身份标识，浏览器或爬虫\n",
    "- 反爬机制\n",
    "    - UA检测：判定是不是浏览器，如果不是则异常的访问请求\n",
    "    - User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "- get方法的参数\n",
    "    - url\n",
    "    - params\n",
    "    - headers\n",
    "- get方法的返回值\n",
    "    - response\n",
    "- response的属性\n",
    "    - text：返回字符串形式的响应数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 需求：page_text取豆瓣中的电影详情数据\n",
    "    - url：https://movie.douban.com/typerank?type_name=%E7%A7%91%E5%B9%BB&type=17&interval_id=100:90&action=\n",
    "    - 动态加载的数据\n",
    "- 滚轮拖动到页面底部，发起ajax请求\n",
    "    - 对url发请求，获取不到动态加载的数据\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 动态加载的数据\n",
    "    - 对一个网站爬取的时候，写代码之前需要分析校验想要的爬取的数据是否是动态加载的数据\n",
    "        - 如果是动态加载的数据\n",
    "            - 不是通过浏览器地址栏url请求到的数据，而是另外一个请求到的数据\n",
    "            - 可见不可得\n",
    "            - 基于抓包工具进行全局搜索，锁定到动态加载的数据包，从数据包中可以提取请求的url、请求方式和请求参数\n",
    "        - 不是动态加载的数据\n",
    "            - 浏览器地址请求到的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://movie.douban.com/j/chart/top_list'\n",
    "params = {\n",
    "    'type': '17',\n",
    "    'interval_id': '100:90',\n",
    "    'action': '',\n",
    "    'start': '0',\n",
    "    'limit': '100',\n",
    "}\n",
    "responce = requests.get(url=url, headers=headers, params=params)\n",
    "page_text = responce.json()  # json返回的是序列化好的对象\n",
    "# 解析电影名称和评分\n",
    "for dic in page_text:\n",
    "    name = dic['title']\n",
    "    score = dic['score']\n",
    "    print(name+':'+score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 问题：如果检测页面中的数据是否为动态加载的数据\n",
    "    - 基于抓包工具进行局部搜索\n",
    "        - 能搜索到：不是动态加载的数据\n",
    "        - 不能搜索到：是动态加载的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 肯德基餐厅查询：http://www.kfc.com.cn/kfccda/storelist/index.aspx\n",
    "- 分析：\n",
    "    - 数据为动态加载的数据\n",
    "    - 通过抓包工具的全局搜索捕获动态加载数据\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "育慧里\n",
      "京通新城\n",
      "黄寺大街\n",
      "四季青桥\n",
      "亦庄\n",
      "石园南大街\n",
      "北京南站\n",
      "北清路\n",
      "大红门新世纪肯德基餐厅\n",
      "巴沟\n",
      "亦庄沃尔玛\n",
      "通州北苑华联\n",
      "西环嘉茂\n",
      "日照银座餐厅\n",
      "荆州北京路\n",
      "荆州月亮湾\n",
      "金山\n",
      "北京南站三\n",
      "北京南站六\n",
      "马坡汽车穿梭\n",
      "新城外城DT\n",
      "铁路（西单商场）\n",
      "苏州路\n",
      "蒙苑\n",
      "南昌\n",
      "太原府东\n",
      "金利\n",
      "芜湖营盘山路\n",
      "钢铁大街\n",
      "阜阳火车站\n",
      "淮安北京路苏果\n",
      "小河\n",
      "金阳82266155\n",
      "国酒路\n",
      "白云\n",
      "北辰\n",
      "购物广场\n",
      "博乐友好\n",
      "梦时代\n",
      "荆州万达\n",
      "洪泽大润发\n",
      "洪城大厦\n",
      "福润德餐厅\n",
      "奎屯友好餐厅\n",
      "徐州泉山大润发\n",
      "大兴龙湖\n",
      "徐州铜山万达\n",
      "银川悦海\n",
      "十堰万达\n",
      "银川新华联\n",
      "沧州高铁站\n",
      "北海宁春城\n",
      "宣堡服务区北\n",
      "西郊百益餐厅\n",
      "仰忠汇\n",
      "南昌甜品站\n",
      "北京肯德基有限公司紫码路餐厅\n",
      "拉萨功德林天街\n",
      "橘洲一号\n",
      "十堰远洋国际\n",
      "新机场2号\n",
      "胜利门\n",
      "观山湖印象城\n",
      "沭阳万达\n",
      "白云独立外卖点\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword'\n",
    "for page_num in range(1, 8):\n",
    "\n",
    "    data = {\n",
    "        'cname': '',\n",
    "        'pid': '',\n",
    "        'keyword': '北京',\n",
    "        'pageIndex': str(page_num),\n",
    "        'pageSize': '10',\n",
    "    }\n",
    "    # 参数data是用来实现参数动态化的，等同于get方法中的params\n",
    "    responce = requests.post(url=url, headers=headers, data=data)\n",
    "    page_text = responce.json()\n",
    "    for dic in page_text['Table1']:\n",
    "        name = dic['storeName']\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 需求：爬取药监总局中的企业详情数据，每一个企业详情页对应的详情数据\n",
    "- url："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "陈飞\n",
      "朱金平\n",
      "郑进嘉\n",
      "邹异虎\n",
      "田雪峰\n",
      "陈永坚\n",
      "李秀曼\n",
      "高伟\n",
      "林云崖\n",
      "廖盛华\n",
      "陈上松\n",
      "陈玉梅\n",
      "肖伟荣\n",
      "张伟\n",
      "黄祖源\n",
      "李虹\n",
      "吴茂瑜\n",
      "邹鼎全\n",
      "李俊锋\n",
      "徐风云\n",
      "黄炜\n",
      "张好学\n",
      "管良富\n",
      "尹朝明\n",
      "李锦涛\n",
      "周娟\n",
      "李勇\n",
      "郑丽华\n",
      "肖慰南\n",
      "黄荣发\n",
      "翁晓锋\n",
      "阮仁全\n",
      "李俊杰\n",
      "邓海燕\n",
      "韩阳阳\n",
      "刘培\n",
      "涂世君\n",
      "黄立剑\n",
      "翟培君\n",
      "邓建明\n",
      "李圣根(Lee Sung Kun)\n",
      "范群\n",
      "刁福平\n",
      "张庆东\n",
      "王岱鹏\n",
      "温俊标\n",
      "曾粮\n",
      "吕磊\n",
      "周勇\n",
      "刘永锋\n",
      "蒋卫东\n",
      "易志红\n",
      "吴苗苗\n",
      "吕小明\n",
      "梁克栋\n",
      "晏卫祥\n",
      "刘璇\n",
      "陈创裕\n",
      "蔡海英\n",
      "张志强\n",
      "吴金泉\n",
      "唐建忠\n",
      "杨颖忠\n",
      "崔胜\n",
      "赵福良\n",
      "孙昌星\n",
      "康龙岳\n",
      "孙杰\n",
      "刘增凡\n",
      "林雪峰\n",
      "肖本大\n",
      "陈明珠\n",
      "张波\n",
      "陈姚森\n",
      "郑宇驰\n"
     ]
    }
   ],
   "source": [
    "url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList'\n",
    "for page in range(1, 6):\n",
    "    data = {\n",
    "        'on': 'true',\n",
    "        'page': str(page),\n",
    "        'pageSize': '15',\n",
    "        'productName': '',\n",
    "        'conditionType': '1',\n",
    "        'applyname': '',\n",
    "        'applysn': ''}\n",
    "    responce = requests.post(url=url, headers=headers, data=data)\n",
    "    page_text = responce.json()\n",
    "\n",
    "    for dic in page_text['list']:\n",
    "        _id = dic['ID']\n",
    "        detail_url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById'\n",
    "        data = {\n",
    "            'id': _id,\n",
    "        }\n",
    "        res = requests.post(url=detail_url, headers=headers, data=data)\n",
    "        detail_text = res.json()\n",
    "        person_name = detail_text['businessPerson']\n",
    "        print(person_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如何爬取图片数据\n",
    "    - 方式1：requests\n",
    "    - 方式2：urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests模块\n",
    "url = 'http://pic.sc.chinaz.com/files/pic/pic9/202006/apic26209.jpg'\n",
    "res = requests.get(url=url, headers=headers)\n",
    "img_data = res.content # content返回的是bytes类型的响应数据\n",
    "with open('./123.png', 'wb') as f:\n",
    "    f.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./456.png', <http.client.HTTPMessage at 0x1d27a6c7688>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://pic.sc.chinaz.com/files/pic/pic9/202006/apic26209.jpg'\n",
    "request.urlretrieve(url=url,filename='./456.png')  # 快速爬取图片数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 问题：两种图片爬取方式的区别\n",
    "    - requests可是实现UA伪装，urllib无法实现UA伪装"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
